---
title: Data Transformations Connector
description: Write-enabled SQL connector for running data transformation tools on Dune
icon: "arrows-rotate"
---

## What is the Data Transformations Connector?

The Data Transformations Connector provides **write access to DuneSQL** via a Trino API endpoint, enabling you to run data transformation tools directly against Dune's data warehouse. Create, update, and manage tables in your private namespace while reading from Dune's comprehensive blockchain datasets.

<Note>
**Looking to use dbt?** Check out our [dbt Connector guide](/api-reference/data-transformations/dbt-connector) for a complete setup guide with templates, CI/CD workflows, and best practices.
</Note>

This connector supports industry-standard SQL operations for data transformations:

- **DDL Operations**: CREATE, DROP, ALTER tables, views, and schemas
- **DML Operations**: INSERT, MERGE, DELETE, TRUNCATE for data manipulation
- **Maintenance**: OPTIMIZE and VACUUM for table performance management

All data stays within Dune — nothing leaves the platform. Your transformations read from Dune's blockchain data and write results to your private, isolated namespace.

## Key Features

### Private Namespace & Data Control

- **Namespace isolation**: Tables are created in your organization's namespace (e.g., `your_team.table_name`)
- **Private by default**: All tables and views you create are private to your team
- **Optional sharing**: Choose to make specific datasets public when appropriate (still WIP, not yet available)
- **No data egress**: All transformations execute within Dune's infrastructure

### Enterprise-Ready Workflows

- **Drop-in integration**: Add to existing data pipelines — works with your current dbt, Airflow, or Prefect setup
- **Git-based workflows**: Manage transformation logic in version control with PR reviews and approvals
- **Production orchestration**: Schedule and automate with the tools you already use
- **Audit & governance**: Private by default with full data lineage tracking and access controls
- **Autonomous deployment**: No waiting for community reviews—deploy on your schedule

### Broad Tool Compatibility

The connector works with any tool that supports the Trino protocol:

- **[dbt](/api-reference/data-transformations/dbt-connector)** via dbt-trino (our first supported use case)
- **Hex** for interactive notebooks
- **Jupyter** for Python-based workflows
- **SQLMesh** for data transformation management
- **Metabase**, **DBeaver**, or any Trino-compatible SQL client

## Getting Started

### Prerequisites

To use the Data Transformations Connector, you need:

- **Dune Enterprise account** with Data Transformations enabled
- **Dune API key** ([generate one here](https://dune.com/settings/api))
- **Team name** on Dune (defines your namespace)
- A **Trino-compatible tool** (dbt, Hex, Jupyter, SQLMesh, etc.)

### Connection Details

Connect to Dune using these parameters:

| Parameter | Value |
|-----------|-------|
| **Host** | `trino.api.dune.com` |
| **Port** | `443` |
| **Protocol** | HTTPS |
| **Catalog** | `dune` (required) |
| **Authentication** | JWT (use your Dune API key) |
| **Session Property** | `transformations=true` (required for write operations) |

<Warning>
The session property `transformations=true` is **required** for all write operations. Without it, DDL and DML statements will be rejected.
</Warning>

### Tool-Specific Setup

**Tested and supported tools:**

- **[dbt Connector](/api-reference/data-transformations/dbt-connector)** - Full guide for running dbt projects on Dune with templates, CI/CD, and best practices

**Other tools that should work:**
- **Hex** - Connect using Trino data connection with the parameters above
- **Jupyter** - Use `trino-python-client` library with JWT authentication
- **SQLMesh** - Configure Trino connection with transformations session property
- **SQL Clients** - Any Trino-compatible client (DBeaver, Metabase, etc.)
These tools may require additional configuration to work with Dune. Please reach out to [support@dune.com](mailto:support@dune.com) if you need help.

## How It Works

###  Namespace Isolation

All tables and views you create are organized into your team's namespace:

- **Production schema**: `{your_team}` - For production tables
- **Development schemas**: `{your_team}__tmp_*` - For development and testing

This ensures complete isolation between teams and between development/production environments.

### Write Operations

Execute SQL statements to create and manage your data:

1. **Create tables and views** in your namespace
2. **Insert, update, or merge** data using standard SQL
3. **Drop tables** when no longer needed
4. **Optimize and vacuum** tables for performance

All operations are authenticated via your Dune API key and restricted to your team's namespace.

### Querying Your Tables

When querying tables you've created via the connector on the Dune app or Analytics API, use the full three-part naming convention:

**Pattern**: `dune.{schema}.{table}`

```sql
-- ✅ Correct - includes catalog prefix
SELECT * FROM dune.your_team.my_table
SELECT * FROM dune.your_team__tmp_.dev_table

-- ❌ Won't work - missing catalog prefix
SELECT * FROM your_team.my_table
```

<Note>
The `dune.` catalog prefix is required when querying on the Dune app or via the Analytics API. Some SQL clients may omit this in their output, so remember to add it when using queries in Dune.
</Note>

## Where Your Data Appears

Tables and views created through Data Transformations appear in the **Data Explorer** under:

**My Data → Connectors**

<img src="/images/data-transformations-explorer.png" alt="Data Transformations in Data Explorer under Connectors" />

You can:
- Browse your transformation datasets
- View table schemas and metadata
- Delete datasets directly from the UI
- Search and reference them in queries

## Pricing & Credits

Data Transformations is an **Enterprise-only feature** with usage-based credit consumption.

### Credit Consumption

Your credit usage includes three components:

#### 1. Compute Credits
- Same as Fluid Engine credits for query execution
- Charged based on actual compute resources used
- Depends on query complexity and execution time

#### 2. Write Operations
- **Minimum 3 credits** per write operation
- Scales with data volume (GB written)
- Applied to INSERT, MERGE, CREATE TABLE AS SELECT, etc.

#### 3. Storage Credits
- **4 credits per GB per month**
- Calculated based on end-of-day storage usage
- Encourages efficient data management and cleanup

#### 4. Maintenance Operations
- OPTIMIZE, VACUUM, and ANALYZE operations consume credits
- Based on compute resources and data written during maintenance
- Can be automated with dbt post-hooks

<Note>
There is no separate platform fee—you only pay for what you use through credits. See [Billing](/api-reference/overview/billing) for more details on credit pricing.
</Note>

## Use Cases

### Enterprise Data Pipelines

Add Dune to your existing data infrastructure without reworking your workflows:
- **Drop-in compatibility**: Integrate with your current dbt projects, Airflow DAGs, or Prefect flows
- **Full incremental support**: Use merge, delete+insert, or append strategies for efficient updates
- **Production orchestration**: Schedule with the tools you already use (GitHub Actions, Airflow, Prefect)
- **Version controlled**: Keep all transformation logic in Git alongside your other data pipelines

### Governance & Compliance

Meet enterprise requirements for data control and auditability:
- **Private by default**: All datasets remain private to your team unless explicitly shared
- **Audit trails**: Track every transformation through Git history and PR workflows
- **Data lineage**: Maintain clear lineage from raw data through transformations to analytics
- **Review processes**: Implement PR reviews and approval workflows before deploying to production
- **Access control**: Restrict write access to specific teams and namespaces

### Complex Analytics Workflows

Build sophisticated multi-stage data pipelines:
- Read from Dune's comprehensive blockchain datasets across all chains
- Transform and enrich with your proprietary business logic
- Create reusable intermediate datasets for downstream analytics
- Chain multiple transformations into complex data products

### Alternative to Spellbook

Build and maintain custom datasets without community review processes:
- Deploy transformations on your own schedule
- Keep proprietary logic private to your organization
- Faster iteration cycles without PR review delays

## Supported SQL Operations

Data Transformations supports comprehensive DDL and DML operations for managing your tables and data.

See the complete [SQL Operations Reference](/api-reference/data-transformations/sql-operations) for details on:
- Schema and table management (CREATE, DROP, ALTER)
- Data manipulation (INSERT, MERGE, DELETE)
- View operations
- Maintenance commands (OPTIMIZE, VACUUM)
- Information schema queries

## Data Access

### What You Can Read

- **All public Dune datasets**: Full access to blockchain data across all supported chains
- **Your uploaded data**: Private datasets you've uploaded to Dune
- **Your transformation outputs**: Tables and views created in your namespace

### What You Can Write

- **Your team namespace**: `{team_name}` for production tables
- **Development namespaces**: `{team_name}__tmp_*` for dev and testing
- **Private by default**: All created tables are private unless explicitly made public

### Access Control

- Write operations are restricted to your team's namespaces only
- Cannot write to public schemas or other teams' namespaces
- Schema naming rules enforced: no `__tmp_` in team handles

## Rate Limits

Rate limits for Data Transformations align with the Dune Analytics API:

- Requests are subject to the same rate limiting as API executions
- Large query operations run on the Large Query Engine tier
- See [Rate Limits](/api-reference/overview/rate-limits) for detailed information

## Best Practices

### Schema Organization

- Use `dev` target with personal suffixes during development
- Keep `prod` target for production deployments only
- Consider separate schemas for different projects or domains

### Credit Optimization

- Use incremental models to reduce compute and write costs
- Monitor credit consumption in your usage dashboard
- Drop unused development tables regularly

### Data Management

- Implement table lifecycle policies
- Clean up temporary/test data in `__tmp_` schemas
- Document table retention requirements

### Version Control

- Store all transformation logic in Git
- Use meaningful commit messages
- Tag production releases
- Review PRs before merging to main

## Limitations

### Metadata Discovery

Limited support for some metadata discovery queries like `SHOW TABLES` or `SHOW SCHEMAS` in certain contexts. This may affect autocomplete in some BI tools.

**Workaround**: Use the [Data Explorer](/web-app/query-editor/data-explorer) or query `information_schema` directly.

### Result Set Size

Large result sets may timeout. Consider:
- Paginating with `LIMIT` and `OFFSET`
- Narrowing filters to reduce data volume
- Breaking complex queries into smaller parts

### Read-After-Write Consistency

Tables and views are available for querying immediately after creation, but catalog caching may cause brief delays (typically < 60 seconds) before appearing in some listing operations.

## Getting Help

### Documentation

- [SQL Operations Reference](/api-reference/data-transformations/sql-operations) - Complete list of supported statements
- [DuneSQL Trino Connector](/api-reference/overview/dunesql-trino-connector) - General Trino connection guide
- [GitHub Template Repository](https://github.com/duneanalytics/dune-dbt-template) - Example implementation
- [Query Engine Overview](/query-engine/overview) - DuneSQL capabilities and functions

### Support

- **Dune Discord**: Join our [Discord community](https://discord.gg/ErrzwBz) for community support
- **Enterprise Support**: Contact your account team for dedicated assistance
- **GitHub Issues**: Report bugs or request features in the template repository

## Next Steps

1. **Try the template**: Clone [dune-dbt-template](https://github.com/duneanalytics/dune-dbt-template)
2. **Review SQL operations**: Check the [SQL Operations Reference](/api-reference/data-transformations/sql-operations)
3. **Explore examples**: See sample models in the template repository
4. **Join the community**: Share your use cases and learn from others

Ready to build? Start with our [dbt template repository](https://github.com/duneanalytics/dune-dbt-template) and have your first transformation running in minutes.

